# -*- coding: utf-8 -*-
"""190177F_Layer7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U_LTYE6jSYM2mXScKg3MdgAAWJfujz98
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np          # For mathematical calculations
import matplotlib.pyplot as plt  # For plotting graphs
from pandas import Series        # To work on series
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.model_selection import cross_val_score
# %matplotlib inline
import warnings                   # To ignore the warnings
warnings.filterwarnings("ignore")
from google.colab import drive
from google.colab import files

drive.mount('/content/drive')
# train = pd.read_csv('train.csv')
# valid = pd.read_csv('valid.csv')
train =  pd.read_csv(r'/content/drive/My Drive/Colab Notebooks/ML_Project/Layer7/train.csv')
valid =  pd.read_csv(r'/content/drive/My Drive/Colab Notebooks/ML_Project/Layer7/valid.csv')
test = pd.read_csv(r'/content/drive/My Drive/Colab Notebooks/ML_Project/Layer7/test.csv')
train.shape

train.head
missing_matrix = train.isnull()
# Create a heatmap using seaborn
plt.figure(figsize=(16, 8))
sns.heatmap(missing_matrix, cbar=False)
plt.title("Missing Values Heatmap")
plt.show()

missing_counts = train.isnull().sum()
for column, count in missing_counts.items():
    if(count>0):
      print(f"Column '{column}': {count} missing values")

valid.head
missing_matrix = valid.isnull()
# Create a heatmap using seaborn
plt.figure(figsize=(16, 8))
sns.heatmap(missing_matrix, cbar=False)
plt.title("Missing Values Heatmap")
plt.show()

missing_counts = valid.isnull().sum()
for column, count in missing_counts.items():
    if(count>0):
      print(f"Column '{column}': {count} missing values")

"""**There are some missing values in both train and valid csv files for label 2**"""

# Separate features and labels
X_train = train.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)
y_train = train[['label_1', 'label_2', 'label_3', 'label_4']]
X_val = valid.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)
y_val = valid[['label_1', 'label_2', 'label_3', 'label_4']]
X_test = test.drop(['ID'], axis=1)

X_train.head

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

X_train_scaled_df = pd.DataFrame(X_train_scaled,columns = X_train.columns)

X_valid_scaled_df = pd.DataFrame(X_val_scaled,columns = X_train.columns)
X_test_scaled_df = pd.DataFrame(X_test_scaled,columns = X_train.columns)

X_train_scaled_df.head

#The heat map for corelations of features
plt.figure(figsize=(12,10))
cor =X_train_scaled_df.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.CMRmap_r)
plt.show()

# with the following function we can select highly correlated features

def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if (corr_matrix.iloc[i, j]) > threshold: # we are interested in  coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

corr_features = correlation(X_train_scaled_df, 0.85)
len(set(corr_features))

"""There are no features with having high correlation with each other (cor = 0.85)"""

#Feature Engineering
#Reducting features that has low lasso regression with the label

def lasso_feature_reduction(X_train_scaled,y,feature_names,th_co):
  #GridSearchCV to find the best hyperparameter
  # parameters to be tested on GridSearchCV
  names =feature_names
  params = {"alpha":np.arange(0.00001, 10, 500)}
  # Number of Folds and adding the random state for replication
  kf=KFold(n_splits=5,shuffle=True, random_state=42)
  # Initializing the Model
  lasso = Lasso()
  # GridSearchCV with model, params and folds.
  lasso_cv=GridSearchCV(lasso, param_grid=params, cv=kf)
  lasso_cv.fit(X_train_scaled, y)
  print("Best Params",format(lasso_cv.best_params_))

  # calling the model with the best parameter
  lasso1 = Lasso(alpha=float(lasso_cv.best_params_['alpha']))
  lasso1.fit(X_train_scaled,y)

  # Using np.abs() to make coefficients positive.
  lasso1_coef = np.abs(lasso1.coef_)

  # plotting the Column Names and Importance of Columns.
  plt.figure(figsize=(20, 6))
  plt.bar(names, lasso1_coef)
  plt.xticks(rotation=90)
  plt.grid()
  plt.title("Feature Selection Based on Lasso")
  plt.xlabel("Features")
  plt.ylabel("Importance")
  plt.show()
  feature_subset=np.array(names)[lasso1_coef>th_co]
  return feature_subset

def model_hyperparameter_tuning(model,params, X_t, y_t):
  #hyper parameter tuning
  model_cv=GridSearchCV(estimator=model, param_grid=params)
  model_cv.fit(X_t,y_t)
  return model_cv.best_params_

def check_model_accuracy(model,X_t, y_t,X_v,y_v):
  model.fit(X_t, y_t)
  y_pred = model.predict(X_v)
  #Accuracy using validate data set
  print("Accuracy from validation : ", accuracy_score(y_v, y_pred))

  #Cross Validation
  acc_cv = cross_val_score(model, X_t, y_t, cv=3).mean()
  print("Accuracy from cross validation trainig",acc_cv)

"""**Label 1**"""

import seaborn as sn
plt.figure(figsize=(12, 6))
sn.countplot(data=y_train, x='label_1', color='teal')
plt.xlabel('Speaker', fontsize=12)

feature_subset_label1=lasso_feature_reduction(X_train_scaled,y_train['label_1'],X_train_scaled_df.columns, 0.3)
dropped_features_label1 =len(X_train_scaled_df.columns)-len(feature_subset_label1)
print("Dropped features for label 1 using lasso", dropped_features_label1)

X_train_fr_label1 = X_train_scaled_df[feature_subset_label1].values
X_valid_fr_label1 = X_valid_scaled_df[feature_subset_label1].values
X_test_fr_label1 = X_test_scaled_df[feature_subset_label1].values

#Dimensiolity reduction

pca = PCA(n_components=0.95, svd_solver = 'full')

X_train_pca = pca.fit_transform(X_train_fr_label1)
X_valid_pca = pca.transform(X_valid_fr_label1)
X_test_pca = pca.transform(X_test_fr_label1)

X_train_pca_without_fr = pca.fit_transform(X_train_scaled)
X_valid_pca_without_fr = pca.transform(X_val_scaled)
X_test_pca_without_fr = pca.transform(X_test_scaled)

# Randomforest
rf_label1= RandomForestClassifier()
check_model_accuracy(rf_label1,X_train_pca,y_train['label_1'],X_valid_pca, y_val['label_1'])

#Random forest without fearure reduction
check_model_accuracy(rf_label1,X_train_pca_without_fr,y_train['label_1'],X_valid_pca_without_fr, y_val['label_1'])

#SVM
#Hyperparameter tuning

param_grid = {'C': [100, 1000], 'gamma': [0.01, 0.001, 0.0001]}
best_params_svm_l1 = model_hyperparameter_tuning(SVC(),param_grid,X_train_pca,y_train['label_1'])
print(best_params_svm_l1)

#SVM
param_grid = {'C': [10,100], 'gamma':[0.001]}
best_params_svm1_l1 = model_hyperparameter_tuning(SVC(),param_grid,X_train_pca,y_train['label_1'])
print(best_params_svm1_l1)

#SVM
param_grid = {'C': [100], 'gamma':[0.001],'kernel':['linear','rbf','poly']}
best_params_svm2_l1 = model_hyperparameter_tuning(SVC(),param_grid,X_train_pca,y_train['label_1'])
print(best_params_svm2_l1)

svm_label1 = SVC(C=100, gamma=0.001, kernel='rbf')
check_model_accuracy(svm_label1,X_train_pca,y_train['label_1'],X_valid_pca, y_val['label_1'])

#svm without fearure reduction
check_model_accuracy(svm_label1,X_train_pca_without_fr,y_train['label_1'],X_valid_pca_without_fr, y_val['label_1'])

#Logistic regression
#parameter tuning
param_grid_lr = {'C': [0.1, 1.0, 10.0],'solver': ['liblinear', 'lbfgs']}
best_params_lr_l1 = model_hyperparameter_tuning(LogisticRegression(),param_grid_lr,X_train_pca,y_train['label_1'])
print(best_params_lr_l1)

lr_label1= LogisticRegression(C=0.1)
check_model_accuracy(lr_label1,X_train_pca,y_train['label_1'],X_valid_pca, y_val['label_1'])

print("X :",X_train_pca)
print("Y :",y_train['label_1'])

#lr before feature engineering
check_model_accuracy(lr_label1,X_train_pca_without_fr,y_train['label_1'],X_valid_pca_without_fr, y_val['label_1'])

"""Best accuracy is given from SVC for label1

**Label 2**
"""

train['label_2'].isnull().sum()

label2_train = train.copy()
label2_valid = valid.copy()
label2_test = test.copy()

label2_train = label2_train.dropna(subset=['label_2'])
label2_valid = label2_valid.dropna(subset=['label_2'])

label2_train['label_2'].isnull().sum()

X_train_label2 = label2_train.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)
y_train_label2 = label2_train[['label_1', 'label_2', 'label_3', 'label_4']]
X_valid_label2 = label2_valid.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)
y_valid_label2 = label2_valid[['label_1', 'label_2', 'label_3', 'label_4']]
X_test_label2 = label2_test.drop(['ID'], axis=1)

plt.figure(figsize=(12, 6))
sn.countplot(data=y_train_label2, x='label_2', color='teal')
plt.xlabel('Speaker', fontsize=12)

X_train_label2_scaled = scaler.fit_transform(X_train_label2)
X_val_label2_scaled = scaler.transform(X_valid_label2)
X_test_label2_scaled = scaler.transform(X_test_label2)

len(X_test_label2_scaled[0])

X_train_label2_scaled_df = pd.DataFrame(X_train_label2_scaled,columns = X_train.columns)
X_valid_label2_scaled_df = pd.DataFrame(X_val_label2_scaled,columns = X_train.columns)
X_test_label2_scaled_df = pd.DataFrame(X_test_label2_scaled,columns = X_train.columns)

feature_subset_label2=lasso_feature_reduction(X_train_label2_scaled,y_train_label2['label_2'],X_train_scaled_df.columns, 0.1)
dropped_features_label2 =len(X_train_scaled_df.columns)-len(feature_subset_label2)
print("Dropped features for label 2 using lasso", dropped_features_label2)

X_train_fr_label2 = X_train_label2_scaled_df[feature_subset_label2].values
X_valid_fr_label2 = X_valid_label2_scaled_df[feature_subset_label2].values
X_test_fr_label2 = X_test_label2_scaled_df[feature_subset_label2].values

X_train_label2_pca = pca.fit_transform(X_train_fr_label2)
X_valid_label2_pca = pca.transform(X_valid_fr_label2)
X_test_label2_pca = pca.transform(X_test_fr_label2)

X_train_label2_pca_without_fr = pca.fit_transform(X_train_label2_scaled)
X_valid_label2_pca_without_fr = pca.transform(X_val_label2_scaled)
X_test_label2_pca_without_fr = pca.transform(X_test_label2_scaled)

print(len(X_train_label2_scaled[0]))
print(len(X_test_label2_scaled[0]))
print(len(X_train_label2_pca_without_fr[0]))
print(len(X_test_label2_pca_without_fr[0]))

#Random Forest
#parameter tuning
param_grid_rf = {'max_depth': [10,100,500]}
best_params_rf_l2 = model_hyperparameter_tuning(RandomForestClassifier(),param_grid_rf,X_train_label2_pca,y_train_label2['label_2'])
print(best_params_rf_l2)

# Randomforest
rf_label2= RandomForestClassifier(max_depth=100)
check_model_accuracy(rf_label2,X_train_label2_pca,y_train_label2['label_2'],X_valid_label2_pca, y_valid_label2['label_2'])

check_model_accuracy(rf_label2,X_train_label2_pca_without_fr,y_train_label2['label_2'],X_valid_label2_pca_without_fr, y_valid_label2['label_2'])

#SVC
#hyperparameter tuning
param_grid = {'C': [100, 1000], 'gamma': [0.01, 0.001, 0.0001]}
best_params_svm_l2 = model_hyperparameter_tuning(SVC(),param_grid,X_train_label2_pca,y_train_label2['label_2'])
print(best_params_svm_l2)

#SVM
param_grid = {'C': [10,100], 'gamma':[0.001]}
best_params_svm1_l2 = model_hyperparameter_tuning(SVC(),param_grid,X_train_label2_pca,y_train_label2['label_2'])
print(best_params_svm1_l2)

#SVM
param_grid = {'C': [100], 'gamma':[0.001],'kernel':['linear','rbf','poly']}
best_params_svm2_l2 = model_hyperparameter_tuning(SVC(),param_grid,X_train_label2_pca,y_train_label2['label_2'])
print(best_params_svm2_l2)

svm_label2 = SVC(C=100, gamma=0.001, kernel='rbf')
check_model_accuracy(svm_label2,X_train_label2_pca,y_train_label2['label_2'],X_valid_label2_pca, y_valid_label2['label_2'])

#svm without fearure reduction
check_model_accuracy(svm_label2,X_train_label2_pca_without_fr,y_train_label2['label_2'],X_valid_label2_pca_without_fr, y_valid_label2['label_2'])

#Logistic regression
#parameter tuning
param_grid_lr = {'C': [0.1, 1.0, 10.0],'solver': ['liblinear', 'lbfgs']}
best_params_lr_l2 = model_hyperparameter_tuning(LogisticRegression(),param_grid_lr,X_train_label2_pca,y_train_label2['label_2'])
print(best_params_lr_l2)

lr_label2= LogisticRegression()
check_model_accuracy(lr_label2,X_train_label2_pca,y_train_label2['label_2'],X_valid_label2_pca, y_valid_label2['label_2'])

#lr without fearure reduction
check_model_accuracy(lr_label2,X_train_label2_pca_without_fr,y_train_label2['label_2'],X_valid_label2_pca_without_fr, y_valid_label2['label_2'])

"""The best modal for label2 is SVM

**Label 3**
"""

import seaborn as sn
plt.figure(figsize=(12, 6))
sn.countplot(data=y_train, x='label_3', color='teal')
plt.xlabel('Speaker', fontsize=12)

feature_subset_label3=lasso_feature_reduction(X_train_scaled,y_train['label_3'],X_train_scaled_df.columns, 0.003)
dropped_features_label3 =len(X_train_scaled_df.columns)-len(feature_subset_label3)
print("Dropped features for label 3 using lasso", dropped_features_label3)

X_train_fr_label3 = X_train_scaled_df[feature_subset_label3].values
X_valid_fr_label3 = X_valid_scaled_df[feature_subset_label3].values
X_test_fr_label3 = X_test_scaled_df[feature_subset_label3].values

X_train_pca_label3 = pca.fit_transform(X_train_fr_label3)
X_valid_pca_label3 = pca.transform(X_valid_fr_label3)
X_test_pca_label3 = pca.transform(X_test_fr_label3)

rf_label3= RandomForestClassifier()
check_model_accuracy(rf_label3,X_train_pca_label3,y_train['label_3'],X_valid_pca_label3, y_val['label_3'])

#Random forest without fearure reduction
check_model_accuracy(rf_label3,X_train_pca_without_fr,y_train['label_3'],X_valid_pca_without_fr, y_val['label_3'])

#SVM
#hyperparameter tuning
param_grid = {'C': [100, 1000], 'gamma': [0.01, 0.001, 0.0001]}
best_params_svm_l3 = model_hyperparameter_tuning(SVC(),param_grid,X_train_pca_without_fr,y_train['label_3'])
print(best_params_svm_l3)

svm_label3 = SVC(C=100, gamma=0.001, kernel='rbf')
check_model_accuracy(svm_label3,X_train_pca_without_fr,y_train['label_3'],X_valid_pca_without_fr, y_val['label_3'])

"""**Best model for Label 3 is SVM**

**Label 4**
"""

import seaborn as sn
plt.figure(figsize=(12, 6))
sn.countplot(data=y_train, x='label_4', color='teal')
plt.xlabel('Speaker', fontsize=12)

feature_subset_label4=lasso_feature_reduction(X_train_scaled,y_train['label_4'],X_train_scaled_df.columns, 0.1)
dropped_features_label4 =len(X_train_scaled_df.columns)-len(feature_subset_label4)
print("Dropped features for label 4 using lasso", dropped_features_label4)

X_train_fr_label4 = X_train_scaled_df[feature_subset_label4].values
X_valid_fr_label4 = X_valid_scaled_df[feature_subset_label4].values
X_test_fr_label4 = X_test_scaled_df[feature_subset_label4].values

X_train_pca_label4 = pca.fit_transform(X_train_fr_label4)
X_valid_pca_label4 = pca.transform(X_valid_fr_label4)
X_test_pca_label4 = pca.transform(X_test_fr_label4)

rf_label4= RandomForestClassifier()
check_model_accuracy(rf_label4,X_train_pca_label4,y_train['label_4'],X_valid_pca_label4, y_val['label_4'])

svm_label4 = SVC(C=1000, gamma=0.001, kernel='rbf')
check_model_accuracy(svm_label4,X_train_pca_without_fr,y_train['label_4'],X_valid_pca_without_fr, y_val['label_4'])

"""**Test Predictions**"""

#Predicting test values for label1
svm_label1 = SVC(C=100, gamma=0.001, kernel='rbf')
pred_label1 = svm_label1.fit(X_train_pca_without_fr,y_train['label_1']).predict(X_test_pca_without_fr)

#Predicting test values for label2
svm_label2 = SVC(C=100, gamma=0.001, kernel='rbf')
pred_label2 = svm_label2.fit(X_train_label2_pca_without_fr,y_train_label2['label_2']).predict(X_test_label2_pca_without_fr)

#Predicting test values for label3
svm_label3 = SVC(C=100, gamma=0.001, kernel='rbf')
pred_label3 = svm_label3.fit(X_train_pca_without_fr,y_train['label_3']).predict(X_test_pca_without_fr)

#Predicting test values for label4
svm_label4 = SVC(C=1000, gamma=0.001, kernel='rbf')
pred_label4 = svm_label4.fit(X_train_pca_without_fr,y_train['label_4']).predict(X_test_pca_without_fr)

#Output
output_df = test[['ID']]
output_df['label_1'] = pred_label1
output_df['label_2'] = pred_label2
output_df['label_3'] = pred_label3
output_df['label_4'] = pred_label4

output_df.head()

#Output file download

output_df.to_csv('prediction_layer7.csv', index=False)
files.download('prediction_layer7.csv')